1550 Lec 6
When we go back into user mode from kernel mode
Real time performance in computing system can be calculated by determining which implementation performs
the fewest context switches.
Architecting an OS
A Space - an enumeration of all possible choices/decisions/designs
How do we organize the operating system to make it do the things that we need it to do.
If you hear which of these is best "it depends." Depends on what?
Management of CPU time - Scheduling.
Considerations
There is a kernel mode instruction that sets up the interrupt vector.
What does it mean that the operating system is overhead
If the OS starts writing to the same registers as the userspace program, it will corrupt that data. and that
data will not be useful anymore.
Before we need to perform a context switch.
IRET interrupt return. Goes back to user mode.
What do we do if we don't have enough values to store the register.
SW to store in memory.
Is there a convenient place to put the information stored during a context switch. Managed as a stack.
Take those values and put them into ram.
The context is associated with the process. It could be stored in the outgoing process address space. It just
needs to be somehow associated with the processes address space. Doesn't really matter if its in the "OS
stack" or the "Process stack"
Buffered input avoids the cost of having to do context based system calls.
Context switches are expensive ➔ They have been designed to become as cheap as possible.
What am I scheduling.
Need to know what programs are running.
Linked list, stop or start a new program
Process says they are done by making an exit syscall.
Removes itself from queue data structure
EXEC syscall ➔ Replaces a syscall with another syscall.
FORK syscall ➔ says I am creating a new process.
Chose amongst all of our jobs, which one do we want to run next ➔ this is the ==scheduling algorithm ==.
What do I run now.
Walking a linked list and choosing a particular node.
Switch context to the job that I said
Change the program counter to the instruction of the process that I chose.
When does the scheduling algorithm run.
In a monolithic kernel all of the parts are glued together into what we call the operating system. the union of all
of the pieces I need to run an operating system.
Smaller kernel is referred to as the microkernel.
Exo-kernel smaller kernel by virtue of pushing out functionality that does not need to be in the kernel.
Von Neumann, instructions in memory.
A lot of the operating system work is unprivileged work. but a lot of what the operating system does is acting on
the decision to go into the kernel and do unprivileged work.
Drawback of monolithic kernel.
If you inject code into user space, we can only do the stuff that a user can do.
The monolithic kernel is more resource efficient.
There is an argument that the microkernel/exokernel is actually larger.
In the microkernel/exokernal. The kernel is used as an intermediary so different parts of the kernel cannot directly
communicate. You might need three different copies of a certain function when moving between different user
mode services.
How do you call a function that is not in your address space. You cant too bad.
Monolithic kernel has the opportunity to deduplicate work.
Modifying the kernel
Building a kernel is not a trivial thing.
When the OS crashes you need to initialize the system
Linux is very well architected in spite of being a monolithic kernel.
Hybridize the microkernel.
The context switch is when we calculate the choice of when to start running.
If you do an exploit, all of the code is part of the same kernel, so you have access to the whole kernel.
Because you have access to the interrupt vector.
You can run whatever you want on interrupt.
The entire kernel is vulnerable
By making the kernel smaller its less likely that you have something with a mistake in it.
If we have fewer places we can make mistakes its more likely we got it right.
Build the kernel
Reboot the OS.
Microkernel
change the user space program and stop and start it. No need to restart computer.
Modularity. Little pieces with individual tasks. Most of them are running in user space.
Questions
How something set itself up as the interrupt handler, and "Become the OS".
I am interested in p2p systems, what is my route to learning/understanding them.
What does it mean that the operating system is overhead
Any resource taken by the OS cannot be used by what we really want to run.
Why are context switches expensive?
- because memory is far and we have to do very many of them.
### Memory Management (RAM)

- **We manage RAM because...**

  1. It is a limited resource, like CPU time.
  2. It is volatile (loses content when power is lost).
  3. It is necessary for all programs to do their work, despite the associated cost that comes with it.
  4. Though computers are ultra-powerful, ultimately they are finite.
  5. Thus, we wish to share it.

- **Exclusive Access**
  - Once upon a time, we didn’t have enough RAM to share.
  - Thus, every program had all of the RAM (at least what was leftover after the OS used what it needed) to itself.
  - None of the issues associated with **how we spent the first portion of this course addressing** would exist if we didn’t have to share the RAM.
  - The bounds of partitions must be stored in hardware registers, must be saved, and restored as part of the process.

---

### Degree of Multiprogramming

- The more I/O-bound our workload, the more processes we need to scatter the CPU’s time.
- The more processes we need to run, the more we must share RAM.
- The OS goal is to use all resources there is a demand for.

  - Example:
    - At 80% CPU-bound -> 3 processes.
    - At 20% CPU-bound -> 10 processes.

- **Fixed Partition**: Divide up RAM into chunks of fixed size; allocate new processes to a partition.
  - Partition size probably won’t change with the size of the system, but at least we are sharing now.
  - Partitioning is more difficult than it sounds: **How many partitions? How big will they be?**
  - If you code with absolute addresses, you must ensure you stay within your partition.
  - **This is too much work to solve via OS; would require a context switch on each instr. that deals with an address (all of them basically).**

---

### Options for Handling Deadlock

1. **Ignore**: Ostrich Algorithm.
2. **Detect and Recover**: Too late, no good options. We cannot take resources from processes that hold them without killing them.
3. **Avoidance**: The principle that deadlock is possible, but we will ensure we won’t cause it.
   - Requires knowing future behavior of programs (impossible in dynamic systems), or else it devolves into better scheduling (which is almost guaranteed deadlock-free).
4. **Prevent**: Deadlock is probably “impossible” - a nightmare to implement or design around.
   - At least 1/4 deadlock conditions always false.
   - Almost always, mutual exclusion will apply.
   - Is unable to address preemption or hold/wait.

---

### Four Conditions for Deadlock

1. **Mutual Exclusion**: Resource can only be held by one process at a time.
2. **No Preemption**: This resource cannot be forcibly taken away (CPU time is preemptable, but access to the occupied CR is not preemptable).
3. **Hold and Wait**: A process gains a mutually exclusive resource, holds it, then attempts to gain another, waiting if failed.
4. **Circular Wait**: Process A is waiting for a resource held by B in the CR while D is waiting for A’s resource.
   - If we are preempted in the CR, the resource we are giving is CPU time.

---

### Semaphores

- **Class Semaphore**:

  ```c
  class Semaphore {
      int value;
      Process[] PL;
      void down() {
          value--;
          if (value < 0) {
              PL.enqueue(currentProcess);
              Sleep();
          }
      }
      void up() {
          Process P;
          value++;
          if (value <= 0) {
              P = PL.dequeue();
              Wakeup(P);
          }
      }
  }
  ```

  - If value is negative after `down()` call to wakeup(), we have -1(value) processes asleep.
  - If value is positive, we have a missed wakeup.

- Moving producer/consumer into the OS does not directly solve our issue of deadlock directly. However, if you link against the OS with a semaphore library, we can give the OS the ability to determine who is asleep/asleep and safely make that decision for us.

---

### Producer/Consumer Problem

- From this point forward, we are assuming our chosen solution works.
- **Producer**:
  ```c
  while(1) {
      if (count == N) sleep();
      buffer[in] = …;
      in = (in + 1) % N;
      count++;
      if (count == 1) wakeup(consumer);
  }
  ```
- **Consumer**:
  ```c
  while(1) {
      if (count == 0) sleep();
      out = (out + 1) % N;
      count--;
      if (count == N-1) wakeup(producer);
  }
  ```
- **Shared State**

  1. `int buffer[N]`: Communication channel between the producer and consumer.
  2. `int in = 0, out = 0, count = 0`: all = 0.

- **Condition Variables**
  - `pthread_cond_wait(condition, mutex)`: sleep(), unlock(), and lock().
  - `pthread_cond_signal(condition)`: wakeup().
  ### Page 1

**Title:** We need something that maps Virtual Address → Physical Address

**Mapping:**

- Lots of virtual addresses, less physical addresses (PA)
- Hash!! VA → hash → PA; collisions more than one VA → **Pigeonhole!**
- Hash sucks because we can collide before we need to, same address made but no spot is free. Hash is prescriptive, tells things what to do.
- Collision should happen only when necessary, not prescriptive, we want descriptive.
- Array! Table indexed by VA, contains our PA! Table holds in VA to address table by!!!
- We need to translate at least once per instruction. Sorry.

(Diagram of CPU, VA, MMU, and RAM)

**MMU:** Translates VA to PA and this must be FAST and SMALL. If it gets too big, we may throw on RAM → Disk.

- KBs = Chip
- MBs = RAM
- GBs = lose, don't do Disk

---

### Page 2

**A for effort! E for effort?**

**RAM Limitations:**

- Run length encoding! Sparse graph! If only... Linked List!
- RAM is limited, we must take things I/O as necessary, loading and storing as we go. This is called **Overlay.**

(Diagram showing stack, heap, and memory allocation in RAM)

- Just grab what you need! Should be all good if you need something you don't have in RAM. Assuming we only load main() and main calls f(), we can plop f() over main in RAM while it’s running. Also switch over activation records, etc., keeping what you STILL NEED!! Ship of Theseus!
- Much smaller RAM overhead per process! Each piece comes together to create our full program in smaller pieces.

**Overlay sucks** though, etc... Can we automate?

**Issues:**

- Relocation: Moving in/out is a lot... is our address okay?
- Subset: What do we need in RAM for this process?

(Diagram of 4GB virtual address space and 1GB physical RAM)

- I imagine we want to load some part of program (stack, instructions, etc.) into RAM, but the instruction lies at `0x2004` & `0xFFFFF8`. What do we put them at for RAM? `0x2004` is valid for RAM but `0xFFFFF8` does not fit, so we must relocate. HW is still using the old address!!

---

### Page 3

- **Relocation:** Idea process - changing location in memory over time.
- Checking bounds for RAM partitions is hard! Every instruction throws some pointer twice (TBL). Checking registers every cycle, throwing exception if kept. Much easier to use BASE as a base, and treat all addresses as relative to base.

**Swapping:**

- Swapping is where we just move things (processes) in and out of RAM as necessary to fit everything. Still sort of partitions, but a little less fixed.

(Diagram of swapping structure)

- This takes a process out of the ready queue but keeps it out of RAM. Need a memory scheduler for this also.

**Relocation!!** With fixed partition, at least we moved in between execution. Now, our addresses are kept!! This sucks. Might just use base again. Much worse than batch partitioning.

- Inevitably we must know what is free & what is not. Registers, stack is also pretty, just used whatever we like. Heap & RAM though, there are gaps!! Would fragment HELLA by not using our free space, why does this mean...

**Bitmap:** Direct line use, check on what is free. Bytes? Pages? Where will it be? 4KB of RAM for 64-byte... fragmentation.

- **External Frag:** When we have odd sized chunks (tiny), those spread out.
- **Internal Frag:** When our chunks are too big, wasting space on allocation 4KB Page. Funsies!

---

### Page 4

**Circular Wait**

- Processes (B holds 2), (C wants 1) → (A wants 2), (B holds 2), possibility to deadlock, ID by order (accessed)

- Sometimes, the best option is to leave things alone.

- Deadlock is rare! Why go batsh for something non-game-changing?

**MEMORY**

- RAM is limited. It is volatile. It is necessary, under Von Neumann architecture.

**Divisions:**

- How do we divide it? Maybe fixed partitions? Limited.

(Diagram showing memory layout)

**Absolute:**

- J-Type and I-Type instructions
- J label → `$PC` ← label << 1 (program counter)
- `BNC` same idea ← `$PC` ← `$PC+1(label << 2)`

**THIS** is scary, `J` takes **ABSOLUTE** could be good for relocating to take a definite address.

- How do we know if an address is out of bounds? Check ALL the time CPU! Want to bake it into the CPU, base & limit registers to work against
November 11th, 2024

CS1550 Notes

**Deadlock Conditions:** When these four things occur, deadlock is occurring.

- Mutual exclusion
- Hold and wait
- No preemption
- Circular wait
  - Example: Process A holds the lock and is sleeping, and process B needs the lock before it can wake process A up.

Resources are involved in deadlocks. Achieving mutual exclusion of critical region: resource. Hardware devices may also be resources.

Mutual exclusion and preemptions are properties of resources. Mutually exclusive things cannot be shared.

**What does it mean to say something isn’t preemptable?**

- (Example: What does it mean to preempt the printer?)
- What device has we managed with preemption? The CPU.
- We can take away CPU time but we can’t take away a printer. Same with a disc burner, it cannot be interrupted mid burning.
- By making a deadlock, we defined the CPU as unpreemptive. We developed a begin and end critical region to disable preemption. But we said we shouldn’t do this!
- Preemption of the CPU isn’t the problem. What isn’t preemptable is the critical region, that other processes want to get into.
- Mutually exclusive: Cannot be shared, cannot be preempted (the exclusive access cannot be taken away while you are still in it.) This has nothing to do with disabling CPU preemption.

**Why is your process waiting for a resource?**

- Another process might have it. Why can’t we use it at the same time? The resource we are waiting for must be mutually exclusive, and it cannot be preempted to be taken away.
- This could lead to circular wait. Process A is waiting for a resource held by Process B, which is waiting for a resource held by process A.

What could an OS do about deadlocks?

- Ignore it. Ostrich Algorithm, pretend like it didn’t happen.
- Detect and recover.
  - Make a graph by connecting who is depending on each other. (DFS search to look for cycles. If there’s a cycle, there’s a deadlock if they’re both blocked because of each other.)
  - Kill it if there’s no good options.
  - Running a program from the start again, it may not perform the same way again.
- Avoid it.
  - Requires knowing future behavior of a program otherwise devolves into batch schedule (already deadlock free.)
  - Banker’s algorithm: If anyone asks for resources, always have enough to satisfy them. Need to know what resources they need and when, which is impossible because we can’t tell the future! 😭
- Prevent
  - Make at least one of the four conditions always false.
    - Can’t make hold and wait or no premption always false.
  - Mutual exclusion via spooling.
    - Loops imply a backedge - a cycle. If the OS prevents moving ‘downwards’ in the graph, it will reject cycles. Resources, if numbered, can only be asked for in ascending order.
      - How would we number this?!

What about the costs vs benefits?

- If it’s rare, sometimes the best course of action is to do nothing at all.

Memory Management

We need to manage memory because there’s only so much of it.

The more IO-bound our workload, the more processes we need to run to saturate the CPU’s time.

The more processes we need to run, the more we must share RAM.

We can make partitions for processes to share, but their size can’t change while the computer is on.

For partitions, how many slices should we make?

What happens when one queue is very slow, and the others go through fast?

Sharing problems:

- Protection problem: What if a process can see outside its range?
- What if a program moves in RAM?
# Deadlock review
- **Mutual Exclusion**
- **Hold and Wait**
- **No** **preemption**
- **Circular** **wait**

- Last couple of weeks, deadlock is a potential result of race conditions
- Trying to fix race conditions can cause deadlock.
- "Resource" is exclusive access to critical region

- Our resource has to be mutually exclusive
	- If one process has the resource no other process can have it.
	- Deadlock can occur because of critical regions
	- A printer is mutually exclusive.
- resources are non preempt-able.
	- If the resources are not preempt-able, deadlock could occur.
	- Resources cannot be forcibly taken away.
	- preempt-ability means, that A critical region should be able to be interrupted.
	- If the process sleeps in the critical region it would depend on something else to wake them up
- one process enters the critical region and goes to sleep.
	- Just this does not cause deadlock
- Why are critical regions involved in deadlock?

- Ive entered the critical region
- I go to sleep
- That is not deadlock
- When another process wants to enter the critical region.
- It cannot because the other process never left the critical region.

- circular wait requires two processes running at the same time
### What is preemptable
- Taking a resource that a process currently has and giving it to someone else.
- CPU being preemptable is not the same as the critical region is not premptable.
	
- Is the CPU's time preemptable, yes
- Is the Critical region preemptable no
- You need to balance the preemption of the CPU with the need for lack of preemption in the critical region. The OS acts as a third party.
- A printer is not 
	- You cannot stop printing halfway through and start printing something else.
- A disk burner is not
	- Only 1 disk at a time.
- Preemption is an ingrained property of the resource.
	- when you buy a printer it is preemptable and it cannot change.

- Hold and wait programs could get into a deadlock.
### What does the OS do about deadlock
- Idempotent: repeatable without side affects.
	- Atomic could be a synonym.
	- x++ is not idempotent.

 - Ostrich algorithm: Ignore a problem and pretend its not there
 1. Ignore
	 - Ostrich algorithm, pretend like the problem didn't happen.
 2. Detect and recover
	 - Too late, no good options.
	 - you could use DFS to detect if you have a cycle, because circular wait is a condition for deadlock.
	 - We don't want the operating system to kill processes.
3. Avoid
	-  Requires knowing future behavior of a program otherwise devolves into batch schedule (already deadlock free)
4. Prevent
	-  Make at least one of the four conditions always false
	-  Probably mutual exclusion via **spooling**.

- Avoidance says, deadlock could still happen but we are gonna try very hard to make sure it not happen.
	- Problematic because we do not know the future.
- Prevention, deadlock is mathematically/provably impossible to occur.

- Bankers Algorithm: based off how savings and loans used to work. 
	- Keep around a small amount of income so that if people come in and ask for their money, you have it.
	- This is essentially a scheduling algorithm?
- Batch schedules are already deadlock free.

- Databases can end up in deadlock
	- Databases use two phase locking
		- Aquire all of your resources in phase1
		- Move one to phase 2 where you do what you want.
	- If you cant get all of your resources, release all of the resources that you hold.
		- You either get all as one group 
		- or release them all and try again later to get them as one group

- Deny the ability to go to a number resource lower then the resource you are currently on.
	- This prevents the cycle in our two processes, and prevents race conditions?
 - You cant request some resource numbered lower then the number of the highest resource that you currently hold.
   - If you actually made a system based off global numbering, it would be a mid operating system to write code in.

- If deadlock is rare and the solution is rebooting your computer
	- And rebooting your computer happens more often then deadlock.
- Just do nothing about deadlock, because its uncommon
- Sometimes the solution is worse then the problem. use ostrich algorithm.
- You haven't seen that many deadlocks.
- It is possible to have a deadlock because of a lack of a number of resources (resource acquisition). The solution is to add more resources.

END OF MANAGING THE CPUS TIME
# Memory management
- Why do we manage memory (RAM)?
	- is a limited resource
	- it is volatile has an associated cost to it, but its needed form programs to do their work.
- Every time we build a new type of computer system, we find new ways of using RAM.
- Under the assumption that the demand for RAM, is greater than its supply.
- Von Neumann requires us to put code and data into RAM.
- 1 process in 1 RAM, we can do whatever we want.

- The more IO-bound our workload, the more processes we need to run to saturate the CPU’s time
- The more processes we need to run, the more we must share RAM.
	- It must be split into more pieces.
- In reality, you have 300 programs, and basically no CPU utilization.
	- Take RAM and split it up into 300 pieces in order to maintain a high degree of multiprogramming
	- Von Neumann says that every process needs to be in RAM.
#### Imagine fixed partitions
- We can divide up RAM (partition) into chunks of fixed size and allocate new processes to one of these partitions. 
- Partition size probably can’t change while the system is on.

- More partitions: each one is smaller
- larger partitions: you have less of them overall.

- How do you assign jobs that want to run, to specific partitions. How do you know what size memory block a process will have. 
	- you don't want to give a big partition to a small process
	- Later you might get a big process that needs the big partition.

#### Protection Problem
- Make sure that process does not read or write memory that belongs to another.
- Several solutions to this problem.

- Know the bounds of the current partition and check.
#### Relocation Problem
- Absolute addresses, create problems between executions, if we've moved.
- What if a program were to move in RAM?
- When could it move?
	- Between executions.
	- During execution.

## Questions
- Look more specifically at instances where deadlock can occur
- Look into database deadlocks and two phase locking.
- Why does DFS detect cycles but BFS does not?
- Circular wait is an interesting condition
- Loop is defined by presence of a "back edge"
	- Go somewhere and reach back to where you were before
- How does this "ability to go to a resource numbered lower then the resource that your process currently has" prevent, deadlock/race conditions.
- The greed of "one instruction" infinite loop is called "live-lock"
- Lol what is the runtime of the ostrich algorithm.
![[CS 1550 Memory New.pdf]]

![[CS 1550 Intro to OS.pdf]]
markdown-to-html: <span style="font-weight:bold;color:rgb(0,187,0)">html/1550_Lec_19.html</span>
# 11 / 11 - Lecture 20

## Memory Management

We manage RAM because it is a **limited, volatile** resource
As we run more and more processes at the same time, we must share more and
more RAM

### Naive Solution

Give all of RAM to any process that is running
This “solutionˮ kills concurrency

### Fixed Partitions

Divide RAM into chunks of fixed size and then allocate them to processes that
want to run
Partition size cannot change while the system is on
This solution reintroduces protection and relocation problems back into our code
Protection  A process can look into the memory of another process. This is
unsafe and could be abused by malicious attackers

```
Relocation  The memory a process is allocated gets changed over time. The
relocation problem is the issue of a program not expecting this move and using
a memory address that mightʼve belonged to it in the past, but does not
anymore
How do we solve these problems:
For protection, we set up bounds for every memory region. If a process
wants to use memory outside of its bounds, we deny it
```

```
In this approach, every instruction that a program runs, a context switch
occurs just to check if the program is in bounds. Some instructions context
switch more than once because they have default context switches already
associated with them (for example, sw and lw).
This is unusable due to poor performance
To make this approach actually usable, weʼd have to add two registers to
our physical CPU architecture since that would be the only way we have
a fast enough way to check memory bounds.
To solve the relocation problem, we must use relative memory addresses
over absolute ones.
Our base is always going to be address 0x0 in this approach
```

### Swapping

A way of creating an illusion of more memory is **swapping. Swapping** is the idea
of “pausingˮ a process by putting it away in memory to let some other processes
run instead.
This approach has a multitude of problems:
Protection and Relocation have been reintroduced
A program can be brought back from memory and might try to access a
memory address that is no longer valid for it
Dynamic allocation becomes more difficult because we donʼt know the exact
amount of memory a process will need
How do we keep track of what memory is free and what is in use?

## Free Space Tracking

Bitmap
Free vs in-use are 2 mutually exclusive options
Store 1 bit per “chunkˮ

```
Process A gets paused in step 4
```

Where is this data structure stored?
In the same place as the memory it is managing
Somewhere in RAM or Disk based on what space we are tracking
If we assume that the smallest addressable unit is a byte and we want to
track free memory at the smallest unit:
1 bit needed for every byte. In other words, for every 9 bits, there is 1
usage bit
11% of our RAM is going to go towards just storing the bitmap
This is inefficient
Letʼ s increase the number of bytes covered by the bitmap. Now, one bit
represent 1 Megabyte:
1 bit needed for every megabyte
This approach gives us an overallocation problem where someone only
requests a small amount of memory but is given a whole megabyte
chunk
The wasted space that is not used because it is not needed is called
**Internal Fragmentation**

```
Large chunk sizes cause wasted space
Small chunk sizes mean our bitmap is going to take up more space
Since we have issues with both large and small chunk sizes, we must
experiment with both options to find what works best in practice
```

The average chunk size in practice is 4 kilobytes
### **Mapping Virtual to Physical Addresses**

- **Mapping Overview**:
  - Lots of Virtual Addresses (VAs), fewer Physical Addresses (PAs).
  - Need a mechanism (hashing or similar) to map VAs to PAs.
  - Collisions (e.g., pigeonhole principle) are inevitable.
    - Hashing could work, but collisions are undesirable unless absolutely necessary.
    - We want prescriptive mapping, not random or overly dependent on free space.
  - **Goal**: Translate at least one VA per instruction with minimal performance overhead.
  
- **Constraints**:
  - **Small and Fast**: Mapping must be fast and efficient to avoid becoming a bottleneck.
  - Should reside on-chip because off-chip solutions would slow down access speeds.

---

### **Example: 32-bit System**
- VA = 32 bits → \( 2^{32} = 4 \text{GB} \)
- PA = 32 bits → \( 2^{32} = 4 \text{GB} \)
- Issue: What if we need more than 4GB of RAM?
  - Potential solution: Multiple address spaces or segmentation.
- Calculations:
  - 1GB of RAM requires \( 2^{30} \) addressable units.
  - This is a challenge for a 32-bit architecture if we surpass 4GB.

---

### **Multi-Level Page Table (MLPT)**

- **Algorithm for Translating VA to PA**:
  1. Use the VA to find the **root node** of the MLPT.
  2. Traverse through levels to locate the **frame** or **leaf** where the PA resides.
  3. Compute the offset from the frame to complete the translation.

- **Assumptions**:
  - 2-Level Translation.
  - Time complexity = \( O(2) \) for this example.

---

### **Subsetting (Page Replacement)**

- **Boot-Up Process**:
  - Scheduler assigns the program's first PC (Program Counter) to the Memory Management Unit (MMU).
  - The MMU translates the VA to a PA.
  - If valid: Access memory.
  - If invalid: Trigger a page fault and invoke the OS for page replacement.
    - OS needs to check whether to load the page into RAM or replace an existing page.
    - Replacement depends on whether data is modified (dirty bit).

---

### **Virtual Address Space (VAS)**

- **Exploiting Sparsity**:
  - Linked lists (or similar structures) can be used for sparse VAS mappings.
  - Issues:
    - Linked lists introduce \( O(n) \) traversal overhead.
    - Better alternatives: Balanced trees (\( O(\log n) \)).
    - Multi-Level Page Tables (MLPT) are a more practical solution.

---

### **Multi-Level Page Table Explanation**
- Example: \( 4 \text{GB} \) Virtual Address Space.
  - Each leaf maps to 4KB of RAM.
  - Leaves are omitted when unnecessary.
  - Root size is proportional to the VAS; large VAS means large roots.
  - Efficient for sparse mappings by avoiding leaves when possible.

---

### **Challenges in VA to PA Translation**

1. **Frame Offset**:
   - Page frame gives the starting PA.
   - Offset calculation determines the exact location within the frame.

2. **Sparse Mapping**:
   - Many VAs may not correspond to active PAs.
   - Valid bit optimization:
     - Only store valid mappings in RAM.
     - Saves space and avoids fragmentation.

3. **Scaling Issues**:
   - Assumptions for VA = PA = 32-bit:
     - Requires efficient translation to handle the scale of \( 2^{32} \).

4. **Frame Size**:
   - Larger frames reduce page table size but increase fragmentation risk.
   - Need to balance between frame size and page table efficiency.

---

### **Key Notes**
- **Huge Translation Problem**:
  - \( 2^{32} \text{VAs} \) to \( 2^{32} \text{PAs} \): Infeasible to manage directly.
  - Sparse mappings mitigate this by storing only necessary translations.

- **Page Table Design**:
  - Goal: Minimize RAM usage while maintaining fast access.
  - Frame size affects table size and must align with system constraints.

s


